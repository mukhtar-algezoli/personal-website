---
title: HuBERT and Wav2Vec2 speaker and phonetic orthogonality subspace analysis
summary: This is my MSc Thesis project (still under work). In this project I investigate how speaker and phonetic information are distributed in the representation space learned by Hubert, and Wav2Vec2 (the current speech SOTA models), and how collapsing those subspaces improve downstream tasks like Automatic speech recognition and speaker adaptation. 
tags:
  - Deep Learning
date: '2016-04-27T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart

# links:
#   - icon: twitter
#     icon_pack: fab
#     name: Follow
#     url: https://twitter.com/georgecushen
url_code: 'https://github.com/mukhtar-algezoli/SSL_Ortho'
url_pdf: ''
url_slides: ''
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

This is my MSc Thesis project (still under work). Self-supervised learning (SSL) models of speech are trained on huge amounts of unlabeled data (purely audio conversations) to learn useful representations of speech audio, they are capable of producing representations for word segments in speech without the need for labeled  data. In this project I investigate how speaker and phonetic information are distributed in the representation space learned by Hubert, and Wav2Vec2 (the current speech SOTA models), and how collapsing those subspaces improve downstream tasks like Automatic speech recognition and speaker adaptation. 
